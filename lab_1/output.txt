
====================================================================================================
ENTROPY ANALYSIS - ALL TEST FILES
====================================================================================================

Filename                                      Size (bytes)    H(Y)         H(Y|X)       Reduction   
----------------------------------------------------------------------------------------------------
pan.txt                                       482,542         5.0392       3.4445       1.5947      
sample.jpg                                    96,205          7.2107       4.9577       2.2530      
sample.zip                                    138             4.0085       1.0138       2.9947      
test1.bin                                     1,048,576       0.0000       0.0000       0.0000      
test2.bin                                     1,048,576       8.0000       0.0033       7.9967      
test3.bin                                     1,048,576       8.0000       8.0000       0.0000      
====================================================================================================

INTERPRETATION:
----------------------------------------------------------------------------------------------------

ğŸ“ pan.txt:
   â€¢ H(Y) = 5.04: Moderate entropy - typical for text or structured data
   â€¢ H(Y|X) = 3.44: Good predictability - strong sequential dependencies
   â€¢ Reduction = 1.59: Significant information gain from context
   â€¢ ğŸ’¾ Compressibility: GOOD - statistical methods work well
   ğŸ” Type: Natural language text (structural patterns in language)

ğŸ“ sample.jpg:
   â€¢ H(Y) = 7.21: High entropy - diverse byte distribution
   â€¢ H(Y|X) = 4.96: Good predictability - strong sequential dependencies
   â€¢ Reduction = 2.25: Significant information gain from context
   â€¢ ğŸ’¾ Compressibility: GOOD - statistical methods work well
   ğŸ” Type: Mixed/structured data

ğŸ“ sample.zip:
   â€¢ H(Y) = 4.01: Moderate entropy - typical for text or structured data
   â€¢ H(Y|X) = 1.01: Good predictability - strong sequential dependencies
   â€¢ Reduction = 2.99: Significant information gain from context
   â€¢ ğŸ’¾ Compressibility: GOOD - statistical methods work well
   ğŸ” Type: Natural language text (structural patterns in language)

ğŸ“ test1.bin:
   â€¢ H(Y) â‰ˆ 0.00: File contains only one unique byte value (constant data)
   â€¢ H(Y|X) â‰ˆ 0.00: Perfect predictability - next byte is deterministic given previous
   â€¢ Reduction â‰ˆ 0.00: Knowing previous byte provides no information (random data)
   â€¢ ğŸ’¾ Compressibility: VERY LOW - nearly impossible to compress
   ğŸ” Type: Constant data (e.g., all zeros/ones)

ğŸ“ test2.bin:
   â€¢ H(Y) â‰ˆ 8.00: Maximum entropy - all 256 byte values appear with equal frequency
   â€¢ H(Y|X) â‰ˆ 0.00: Perfect predictability - next byte is deterministic given previous
   â€¢ Reduction = 8.00: Huge information gain from context
   â€¢ ğŸ’¾ Compressibility: EXCELLENT - differential/pattern encoding highly effective
   ğŸ” Type: Sequential pattern (e.g., 00 01 02 ... FF repeated)

ğŸ“ test3.bin:
   â€¢ H(Y) â‰ˆ 8.00: Maximum entropy - all 256 byte values appear with equal frequency
   â€¢ H(Y|X) â‰ˆ 8.00: No predictability - random/independent data
   â€¢ Reduction â‰ˆ 0.00: Knowing previous byte provides no information (random data)
   â€¢ ğŸ’¾ Compressibility: VERY LOW - nearly impossible to compress
   ğŸ” Type: Random/cryptographic data (e.g., /dev/urandom, encrypted file)

====================================================================================================

....
4 passed in 11.81s
